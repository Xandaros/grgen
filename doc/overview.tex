\chapter{Overview}
\label{chp:overview}

\GrG\ is combined from two groups of components:
The first consists of the compiler \texttt{grgen} and two runtime libraries, which offer the basic functionality of the system;
the compiler transforms specifications of declarative graph rewrite rules into highly efficient .NET-assemblies.
The second consists of the interactive command line \texttt{GrShell} and the graph viewer \texttt{yComp},
which offer a rapid prototyping environment supporting graphical and stepwise debugging of rules that are controlled by sequences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{System Overview}\indexmain{overview, system}\label{systemoverview}

Figure~\ref{figsys} gives an overview of the \GrG\ system components and the involved files.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/OverviewGenerationArtefacts}
  \caption{\GrG\ system components and artifacts}
  \label{figsys}
\end{figure}

A graph rewrite system\footnote{In this context, system is less a grammar rewrite system, but rather a set of interacting software components.} is \emph{defined} by a rule set file (\texttt{*.grg}, written in the rule and computations language), which may include further rule set files, and use zero or more graph model description files (\texttt{*.gm}, written in the model language).
Executable code is \emph{generated} from these specifications by \texttt{GrGen.exe}; 
the generated parts are assisted by the \emph{runtime libraries} \texttt{libGr} and \texttt{lgspBackend}.
Together they form the system that can be used via an \emph{API} by arbitrary .NET-applications, for the processing of a graph-based representation.
One application employing the \indexed{API} is shipped with \GrG, the \emph{shell} application \GrShell.
It may be used interactively, esp. for debugging, or may be scripted (in the shell language, which allows via \texttt{libGr} to execute sequences, a concise language offered to control rule applications).

Figure~\ref{process} depicts the major runtime objects.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\textwidth]{fig/OverviewRuntimeArtefacts}
  \caption{\GrG\ major runtime objects}
  \label{process}
\end{figure}

The central object is the \emph{graph}, it adheres to the specified graph model.
(In general you have to distinguish between a graph model on the meta level, a host graph created as instance of the graph model, and a statically specified pattern graph of a rule that matches a portion of the host graph at runtime).
It may be modified directly by \GrShell, or by a user application.
But typically is it changed by the application of one of the generated \emph{actions}, most often from within a \emph{sequence}, which is parsed into an operator tree and then interpreted, by the graph \emph{processing environment}.
The processing environment maintains the global variables in addition, and modifies the graph directly in case of transaction rollback (based on the graph change events recorded since transaction start).

The \GrShell\ is a readily available execution environment that is sufficient for mapping tasks that require only console or file output.
But even when you want to integrate a graph based algorithmic core into your existing .NET application, 
or are forced to write one because you must supply a long-running application reacting to events from the environment, esp. to user input,
may you be interested in additionally using the \GrShell\ because of its debugging abilities.
Employing the graph viewer \yComp\ it can visualize the graph at any time during execution.
Moreover, it can visualize the application of a rule, highlighting the matched pattern in the host graph, and the changes carried out.
It can do so for actions executed from a sequence, which can be executed step-by-step, under debugger control, highlighting the currently executed rule in the sequence.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Tools}

All the programs and libraries of \GrG\ are open source licensed under \indexed{LGPL}.
Notice that the \yComp\ graph viewer is not a part of \GrG ; \yComp\ ships with its own license granted by \yFiles\ for academic use.
\yComp\ is closed-source and only free for non-commercial use --
this means above all that you are not allowed to ship it with a release of your own commercial software, in contrast to the \GrG\ libraries.

Executing a generated graph rewrite system requires .NET 2.0 or later, compiling and debugging a graph rewrite system in addition requires JAVA 1.5 or later. 
You find the tools in the \texttt{bin} subdirectory of your \GrG\ installation.
%\pagebreak

%-----------------------------------------------------------------------------------------------
\subsection{\texttt{\indexed{GrGen.exe}}} \label{grgenoptions}

\parpic[l] {
\includegraphics[width=48pt]{fig/grgen-256.png}
}
\noindent The \texttt{GrGen.exe} assembly implements the \GrG\ generator.
The \GrG\ generator parses a rule set and its model files and compiles them into .NET assemblies.
The compiled assemblies form a specific graph rewriting system together with the \GrG\ backend.

\begin{description}
  \item[Usage] \begin{tabular*}{\linewidth}{@{}l@{}l}\texttt{[mono] GrGen.exe } & \texttt{[-keep [<dest-dir>]] [-use <existing-dir>] [-debug]}\\
        &\texttt{[-b <backend-dll>] [-o <output-dir>] [-r <assembly-path>]}\\
        &\texttt{[-lazynic] [-noinline] [-profile]}\\
        &\texttt{[-statistics <statisticsfile>]}\\
        &\texttt{<rule-set>}\end{tabular*}
    \emph{rule-set} is a file containing a rule set specification according to Chapter~\ref{chaprulelang}. Usually such a file has the suffix \texttt{\indexed{.grg}}. The suffix \texttt{.grg} may be omitted.
By default \GrG\ tries to write the compiled assemblies into the same directory as the rule set file. This can be changed by the optional parameter \emph{output-dir}.
  \item[Options] \mbox{}
    \begin{tabularx}{\linewidth}{lX}
      \texttt{-keep} & Keep the generated C\# source files. If \emph{dest-dir} is omitted, a subdirectory \texttt{tmpgrgen$n$}\footnote{$n$ is an increasing number.} within the current directory will be created. The destination directory contains:
\begin{itemize}
  \item \texttt{printOutput.txt}---a snapshot of \texttt{stdout} during program execution.
  \item \emph{Name}\texttt{Model.cs}---the C\# source file(s) of the \emph{rule-set}\texttt{Modell.dll} assembly.
  \item \emph{Name}\texttt{Actions\_intermediate.cs}---a preliminary version of the C\# source file of the \emph{rule-set}'s actions assembly.
	This file is for internal debug purposes only (it contains the frontend actions output).
  \item \emph{Name}\texttt{Actions.cs}---the C\# source file of the \emph{rule-set}\texttt{Actions.dll} assembly.
\end{itemize}\\
      \texttt{-use} & Don't re-generate C\# source files. Instead use the files in \emph{existing-dir} to build the assemblies.\\
      \texttt{-debug} & Compile the assemblies with debug information.\\
      \texttt{-lazynic} & Negatives, Independents, and Conditions are only executed at the end of matching (normally asap).\\
      \texttt{-noinline} & Subpattern usages and independents are not inlined.\\
      \texttt{-profile} & Instruments the matcher code to count the search steps carried out.\\
      \texttt{-statistics} & Generate matchers that are optimized for graphs of the class described by the \emph{statisticsfile} (see \ref{custom} on how to save such statistics).\\
      \texttt{-b} & Use the backend library \emph{backend-dll} (default is LGSPBackend).\\
      \texttt{-r} & Link the assembly \emph{assembly-path} as reference to the compilation result.\\
      \texttt{-o} & Store generated assemblies in \emph{output-dir}.
    \end{tabularx}
  \item[Requires] .NET 2.0 (or above) or Mono 1.2.3 (or above). Java Runtime Environment 1.5 (or above).
\end{description}

\begin{note}
Regarding the column information in the error reports of the compiler please note that tabs count as one character.
\end{note}

\begin{note}\label{note:modelruledump}
The grgen compiler consists of a Java frontend used by the C\# backend \texttt{grgen.exe}.
The java frontend can be executed itself to get a visualization of the model and the rewrite rules,
in the form of a dump of the compiler IR as a .vcg file:\\
\texttt{java -jar grgen.jar -i yourfile.grg}
\end{note}

\begin{note}
If you run into \texttt{Unable to process specification: The system cannot find the file specified} errors, 
you may need to install a JDK to a non system path and add the bin folder of the JDK to the path variable.
(Normally just installing a JRE is sufficient.)
\end{note}

%\pagebreak

%-----------------------------------------------------------------------------------------------
\subsection{\texttt{\indexed{GrShell.exe}}}

\parpic[l] {
\includegraphics[width=48pt]{fig/grshell-256.png}
}
\noindent The \texttt{GrShell.exe}\indexmain{GrShell} is a shell application on top of the \LibGr.
\GrShell\ is capable of creating, manipulating, and dumping graphs as well as performing graph rewriting with graphical debug support.
For further information about the \GrShell\ language see Chapter~\ref{chapgrshell}.

\begin{description}
  \item[Usage] \texttt{[mono] grShell.exe [-N] [-SI] [-C "<commands>"] <grshell-script>*} \\
     Opens the interactive shell. The \GrShell\ will include and execute the commands in the optional list of \emph{grshell-script}s\indexmain{graph rewrite script} (usually \texttt{*\indexed{.grs}} files) in the given order.
	 The \texttt{grs} suffixes may be omitted. \GrShell\ returns 0 on successful execution, or in non-interactive mode -1 if the specified shell script could not be executed, or -2 if a \texttt{validate} with \texttt{exitonfailure} failed.
  \item[Options] \mbox{}
    \begin{tabularx}{\linewidth}{lX}
      \texttt{-N} & Enables non-debug non-gui mode which exits on error with an error code instead of waiting for user input.\\
      \texttt{-SI} & Show Includes prints out to the console when includes are entered and left.\\
      \texttt{-C} & Execute the quoted \GrShell\ commands immediately (before the first script file). Instead of a line break use a double semicolon \texttt{;;} to separate commands. Take care that an exec inside such a command line needs to be exited with \indexed{\texttt{\#\S}} (to open and immediately close a shell comment, needed as exec terminator, because newline termination is not available here).
    \end{tabularx}
  \item[Requires] .NET 2.0 (or above) or Mono 1.2.3 (or above).
\end{description}

\begin{note}
The shell supports some \texttt{new set} configuration options that map to the \texttt{grgen} compiler flags (see \ref{sec:compilerconfigshell}), use them before any \texttt{new graph} commands so that matchers are generated according to the compiler flags you would use if you would execute the compiler directly.
\end{note}

\begin{example}
The \texttt{-C} option is often not as helpful as needed because bash splits the command given inside "" alongside spaces.
Unless you are a shell wizard in control of escaping and unescaping, you want to work with here documents.
The following example shell script from \texttt{examples/MovieDatabase-TTC2014} shows how to execute an \GrShell-script embedded as here document on all files in a folder.
\begin{grshell}
#!/bin/bash
for file in *.xmi; do	
    GrShell -N << HERE
import movies.ecore $file GrgenifyMovieDatabase.grg
redirect emit $file.grs
exec create_MovieDatabaseModel ;> [create_Movie] ;> [create_Actor] ;> [create_Actress] ;> [create_personToMovie]
redirect emit - 
validate exitonfailure exec noEdgesLeft
quit
HERE
done
\end{grshell}
\end{example}

%-----------------------------------------------------------------------------------------------
\subsection{\texttt{LibGr.dll}}
\label{sct:API}
The \LibGr\indexmain{libGr} is a .NET assembly implementing \GrG's \indexed{API}.
See the extracted HTML documentation for interface descriptions at \url{http://www.grgen.net/doc/API_4_3/};
a short introduction is given in chapter \ref{cha:api}.

%-----------------------------------------------------------------------------------------------
\subsection{\texttt{lgspBackend.dll}}
The \LGSPBackend\indexmain{lgspBackend} is a .NET assembly containing the libGr SearchPlan backend, the only backend supported by \GrG~as of now, implementing together with the generated assemblies the API offered by \LibGr.
It allows to analyze the graph and to regenerate the matcher programs at runtime, on user request, see \ref{custom}.
For a more detailed introduction have a look at chapter \ref{cha:developing}.

%-----------------------------------------------------------------------------------------------
\subsection{\texttt{yComp.jar}}
\label{tools:ycomp}
\yComp\indexmain{yComp} \cite{ycomp} is a graph visualization tool based on \yFiles\ \cite{yfiles}.
It is well integrated and shipped with \GrG, but it's not a part of \GrG.
\yComp\ implements several graph layout algorithms and has file format support for \indexed{VCG}, GML and YGF among others.
\begin{center}
\includegraphics[width=0.45\linewidth]{fig/ycomp1.pdf} \includegraphics[width=0.45\linewidth]{fig/ycomp2.pdf}
\end{center}
\begin{description}
  \item[Usage] Usually \yComp\ will be loaded by the \GrShell. You might want to open \yComp\ manually by typing\\
   \texttt{java -jar yComp.jar [<graph-file>]}\\
  Or by executing the batch file \texttt{ycomp} under Linux / \texttt{ycomp.bat} under Windows,
  which will start \yComp\ on the given file with increased heap space.
  The \emph{graph-file} may be any graph file in a supported format. \yComp\ will open this file on startup.
  \item[Hints] The \indexed{layout algorithm}\indexmainsee{layout}{layout algorithm} \indexedsee{compiler graph}{layout algorithm} (\yComp's default setting, a version of \texttt{\indexedsee{hierarchic}{layout algorithm}} optimized for graph based compiler intermediate representations) may not be a good choice for your graph at hand.
  Instead \texttt{\indexedsee{organic}{layout algorithm}} or \texttt{\indexedsee{orthogonal}{layout algorithm}} might be worth trying.
  Use the rightmost blue play button to start the layout process. Depending on the graph size this may take a while.
  \item[Requires] Java Runtime Environment 1.5 (or above).
\end{description}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Languages and their Features}\indexmain{features}

The process of graph rewriting at runtime can be divided into four steps:
Creating an instance graph according to a model,
searching a pattern aka finding a match,
performing changes to the matched spot in the host graph,
and, finally, selecting which rule(s) to apply where next.

This process is programmed at specification time utilizing a graph model language, a rule and computations language, and the sequences language for controling rule applications.

%------------------------------------------------------------------------------------
\subsection{Graph Model Language}
At the foundation you find the graph model (meta-model) language given with \texttt{class} definitions in the style of an object-oriented language, with some influences of data definition languages as offered by databases. 

It allows you to specify \emph{node} and \emph{edge types}, with \emph{multiple inheritance} on the types (see Chapter \ref{chapmodellang}).
They are used in building a typed, directed multigraph (supporting multiple edges of the same type between two nodes).
In addition, you may declare undirected and arbitrarily directed edges; or may even register external types with \GrG.
Furthermore, connection assertions allow you to restrict the ``shape'' of the graphs.

Node and edge types can be equipped with \emph{typed attributes}, of the commonly known elementary types (integer and floating-point numbers, strings, booleans, and enums) or of some container types (available are hashmap and array based types).
In addition, methods supporting dynamic dispatch may be given.%(they are seldom needed for tasks based on graph-representations, though). 

Moreover, you may configure indices (see Chapter \ref{sec:performance}): attribute indices for a quick lookup of a graph element based on the attribute value, or incidence count indices for a quick lookup based on the number of incident edges.
Several indices are already built-in and automatically used by the pattern matchers, notably the type index for quick lookups based on element type, and the incident elements index giving access to the neighbouring elements in constant time.

%------------------------------------------------------------------------------------
\subsection{Rule and Computations Language}
On top of the graph model language and at the heart of \GrG\ do you find the rule and computations language.

It supports pattern-based \emph{rules}, which are built by a \emph{pattern to be matched} and a \emph{rewrite} to be carried out; or put differently: a left-hand-side precondition pattern and a right-hand-side postcondition pattern (see Chapter \ref{chaprulelang}).
A pattern is described by \emph{graphlets}, a specification of nodes connected by edges in an intuitive syntax.
Complex patterns are combined from elementary patterns with \emph{nested patterns} (defining alternative, optional, multiple and iterated structures, or negative and independent application conditions, see Chapter \ref{cha:nested}) and \emph{subpattern} definitions and usages (see Chapter \ref{cha:sub}).
Besides those parts for declarative processing of structural information are \emph{attribute conditions} (see Chapter \ref{cha:typeexpr}) and \emph{attribute evaluations} available, the former are expressions computing an output value by reading attributes of matched elements, the latter are statements changing attribute values with assignments. 

The attribute processing expressions and statements are in fact parts of a full-fledged \emph{imperative programming language} integrated with the declarative pattern-based rule language (see Chapter \ref{cha:computations}).
The \emph{expressions} can query the graph with a multitude of available functions in addition to reading the attributes of matched elements and carrying out computations with the attributes; they compute a value and can be abstracted into \emph{functions}.
The \emph{statements} support direct graph changes and supply control flow statements and def variable declarations in addition to the attribute assignments; they change the state and can be abstracted into \emph{procedures}.
You especially find subgraph operations for the processing of nested graphs, with subgraph extraction, subgraph comparison, and subgraph insertion (see Chapter \ref{cha:graph}). 

Pattern matching can be adjusted fine-grained with homomorphic matching for selected elements (with \texttt{hom} declarations, so they can match the same graph elements), overriding the default isomorphic matching, and with type constraints (forbidding certain static types or requiring equivalence of dynamic \texttt{typeof}s).

The rewrite part of a rule keeps, adds and deletes graph elements according to the SPO approach, as specified by element declarations and references in the LHS and RHS patterns (there are three additional rule application semantics available: DPO or exact patterns only or induced subgraphs only).
The rewrite part supports two modes of specification: A rule can either express the changes to the match (\texttt{modify}-mode, requiring deletion to be specified explicitly) or specify a whole new subgraph (\texttt{replace}-mode).

A rich bouquet of single element rewrite operations is available on top of the basic ones with \emph{retyping} aka relabeling, that allows to change the type of a graph element while keeping its incident elements, with the creation of new nodes/edges of only dynamically known types or as exact copies of other nodes/edges, and with node merging and edge redirection constructs (see Chapter \ref{chapadvanced}).
Besides changing the graph can you \texttt{emit} user-defined text to \texttt{stdout} or files (supporting model-to-text transformations; see Chapter \ref{cha:imperativeandstate}).

A special class of computations are available with the post-match filter functions (see Chapter \ref{sub:filters}), that allow to inspect and manipulate the set of matches found when applying a rule (with all bracketing). 
This way you can \emph{accumulate} information over the matches, or \emph{filter} the matches set for the most interesting matches.
Several filters for sorting and clipping are already supplied or can be generated automatically on request.
Symmetry reduction is available with a filter for duplicate matches due to permuted elements from automorphic patterns.

Rules and tests (tests are rules with only a LHS pattern) support \emph{parameterization}, with input and output parameters; so do the subpatterns, as well as the functions and procedures.
The parameters are typically used to define and advance the location of processing in the graph, but are also used for attribute computations.
In addition visited flags can be queried and written, for marking already processed elements.

Subpatterns allow for pattern reuse, and allow via \emph{recursion} to match substructures extending into \emph{depth} (e.g. iterated paths -- but for just checking an iterated path condition, i.e. the transitive closure of a relation, there are predefined and even more efficient functions available that can be simply called.)
Iterated patterns allow to match substructures splitting into \emph{breadth} (e.g multinodes).
Both combined allow to match \emph{entire tree like structures} within a single rule application. 
Subpatterns and nested patterns are matched from the outermost to the innermost patterns with \emph{recursive descent}, handing already matched elements down; on \emph{ascend}, they can \texttt{yield} elements found out to the \texttt{def} elements of their containing pattern.
The nested patterns and subpatterns also support \emph{nested rewrite parts}, so that complex structures are not only matched (parsing), but can also get rewritten (transduction) --- yielding \emph{structure directed transformation}, alongside structures defined by graphlets combined in an EBNF-grammar-like way\cite{EBNFAGTIVE}.

A graph rewrite system described in those languages can be employed via an \emph{API} by .NET-applications, typically written in C\# (see Chapter \ref{cha:api}).
%The API is provided in two versions --- for one named and typed entities which get generated, and for the other a string and object based interface on top of it offered by \LibGr. The \LibGr-based API is needed to work with arbitrary systems.
The other way round you can include C\#-code into your specifications by using external types or calling external functions and procedures, as well as sequences (see Chapter \ref{chapextensions}).

%------------------------------------------------------------------------------------
\subsection{Rule Application Control Language}
On top of the rules and computations language do you find the \emph{sequences} language, for controling the application of the rules (and procedures or functions), determining \emph{which rule} to apply \emph{where} next. 

This strategy language allows to determine the rule that is called next by its control operators and constructs (see Chapter \ref{cha:xgrs}).
The basic ones available are the \emph{sequential} and \emph{logical operators}, the \emph{decisions}, and the \emph{loops} (defining the control-flow orchestration).
The location where to apply a rule may be defined with \emph{parameters} handed into the called rules, and received back from them. 
The sequences support \emph{variables} to store the locations (for data-flow orchestration).
A rule may be applied for the \emph{first match} found, or for \emph{all matches} that are available with all-bracketing.

The sequences offer a \emph{computations} sublanguage that supports calling procedures and functions, basic graph querying and modification, visited flags management, and storages manipulation (see Chapter \ref{seqcomp}).
The first ones allow to bypass the rules of the layer below for simple tasks, and especially allow to extract and insert subgraphs. 
Visited flags used in marking processed elements must be allocated and freed, the sequence computations are the right place to do so.
Most usages are related to \emph{storages} which are variables of container type storing graph elements, they esp. allow to build transformations following a wavefront running over the graph (see Chapter \ref{cha:container} for more on containers).

The sequences can be abstracted into a \emph{sequence definition} that can then be called in the place of a rule.
This way common parts can be reused, but especially is it possible to program a recursive strategy.
For simulation runs, several indeterministic choice operators are available.

Advanced constructs are available with \emph{transactions} angles and \emph{backtracking} double angles capable of \emph{rolling back changes} carried out on the graph to get back to the state of the graph again from before their nested content was applied (see Chapter \ref{cha:transaction}).
With them you can easily try things out without having to invest into programming the bookkeeping needed to revert to an old state.
They allow to systematically run through a search space, or even to enumerate a state space, materializing interesting states reached in time out into space.

Those sequences are available in an integrated form in the rule language (see Chapter \ref{cha:imperativeandstate}).
After applying the direct effects of a rule it is possible to apply an \emph{embedded sequence} for follow-up tasks that just can't be expressed with a single rule application; those sequences have access to the elements of their containing rule, they allow to build complex transformations with rule-sequence-rule call chains.
		
%------------------------------------------------------------------------------------
\subsection{Shell Language}
Those were the features of the languages at the core of the \GrG-System (implemented by the generator \texttt{grgen.exe} and the runtime libraries \texttt{libGr} and \texttt{lgspBackend}).
In addition, the \GrG\ system supplies a shell application, the \GrShell,
which offers besides some common commands, variable handling constructs, and file system commands several constructs for \emph{graph handling} and \emph{visualization}, as well as constructs for \emph{sequence execution} and \emph{debugging}.
Furthermore, some switches are available to configure the shell and the graph processing environment, as well as the \texttt{grgen} compiler.

Regarding graph handling you find commands for graph creation and manipulation, graph export and import, and graph change recording plus replaying (see Chapter \ref{chapgrshell}).
For graph creation 3 simple \texttt{new} commands are available, one for creating an empty graph, one for creating a node, and one for creation an edge.
The GRS exporter serializes with the same syntax, which is also expected by the GRS importer.
Write a file in this simple format if the available import formats are not supported by your data source.

For direct graph manipulation you find deletion and retyping commands in addition to the creation commands, and commands for assigning attributes of graph elements.
Moreover, \texttt{export} and \texttt{import} commands are available for serializing a graph to a file, and for unserializing a graph from a file (in \texttt{GRS}, \texttt{GXL} and \texttt{XMI/ecore} formats). 
Changes that are occurring to a graph may be \texttt{record}ed to a \texttt{GRS} and later \texttt{replay}ed or simply \texttt{import}ed.
Change recording allows for post-problem debugging, but allows also to use \GrG\ as a kind of embedded database that persists changes as they appear. %(at the price of replaying changes that are rendered futile by later changes).

The graph and the model can be queried furtheron for e.g. the defined types or the attributes of a graph element.
The graph may be validated against the connection assertions specified in the model, or against a sequence that gets executed on a clone of the host graph.

Regarding sequence execution you find the central \texttt{exec} command that is \emph{interpreting} the following sequence (in fact by delegating the parsing and execution to \texttt{libGr}).
In addition, action \texttt{profiles} may be \texttt{show}n. They are available when \emph{profiling} instrumentation was switched on, telling about the number of search steps or graph accesses carried out.

The actions can be replaced at runtime by loading another actions library; the backend could be, but currently there is only one available, the \texttt{lgspBackend}.
This backend supports further custom commands, that start with a \texttt{custom graph} or \texttt{custom actions} prefix. 
They allow to \texttt{analyze} the graph in order to compile some statistics about the characteristics of the graph that are of relevance for the pattern matcher.
And in the following to re-generate the pattern matchers at runtime (\texttt{gen\_searchplans}) given the knowledge about the characteristics of the graph, yielding matchers which are \emph{adapted to the host graph} and thus typically faster than the default pattern matchers generated statically into the blue.
You can inspect the search plans in use with the \texttt{explain} custom command.

The debugger component of the \GrShell\ allows to \texttt{debug} a sequence \texttt{exec}ution step-by-step, highlighting the currently executed rule in the sequence, and displaying the current graph in the graph viewer \yComp;
in detail mode even the match found and the rewrite carried out by the rule are highlighted in the graph (see Chapter \ref{chapdebugger}).

The graph visualization is highly \emph{customizable}.
You can choose one from several available layout algorithms.
You can define e.g. the colors, the shapes of nodes, or the linestyle of edges, based on the type of the graph elements,
or exclude uninteresting types altogether from the display.
You can pick the attributes that are to be shown directly (normally they are only displayed on mouse-over).
Furthermore, you can configure \emph{visual graph nesting} by declaring edges as containment edges, thus keeping large graphs with containment or tree relations \emph{understandable}.
For large graphs that are beyond the capabilities of the graph viewer you can define that only the matches plus their context up to a certain depth are displayed.
Altogether, you can tailor the graph display to what you need, transforming an unreadable haystack which is what large graphs tend to become into a well-readable representation.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Development Goals and Design Decisions}

\GrG~is the successor of the \textsc{GrGen} tool presented at ICGT 2006~\cite{GBGHS:06}.
The ``.NET'' postfix of the new name indicates that \textsc{GrGen} has been reimplemented in C\# for the Microsoft .NET or Mono environment~\cite{NET,MONO};
it is open source licensed under LGPL3(\url{www.gnu.org/licenses/lgpl.html}) and available for download at \url{www.grgen.net}.

The development goals of \GrG\ have been:

\subsection*{Expressiveness}
is achieved by \emph{declarative} specification languages for \emph{pattern matching} and \emph{rewriting} that are bursting with features, building upon a rich graph model language.

In addition to the unmatched expressiveness of the single-element operations offered by the pattern and the rewrite parts,
are \emph{nested} and \emph{subpatterns} available for combining them flexibly;
they allow to process substructures of arbitrary depth and breadth with a single rule application.
This surpasses the capabilities of the VIATRA2\cite{viatra2,recursiveviatra} and GROOVE \cite{Groove} tools, our strongest competitors regarding rule expressiveness.
You may have a look at the GrGen.NET solution of the program understanding case \cite{ProgramUnderstanding} of the TTC 2011 highlighting how \emph{concise} and \emph{elegant} solutions become due to the expressiveness of the language constructs.

While the patterns can be combined in a functional way with nested and subpatterns to build complex rules, can the rules be combined in an imperative way with graph rewrite sequences, executing a graph state change after the other.

The sequences used for orchestrating the rules are at the time of writing the most advanced \emph{strategy language} offered by any graph rewriting tool, featuring variables of elementary as well as container types (called storages when containing nodes, as pioneered by the VMTS\cite{vmts} tool), and sequential, logical, conditional, and iterative control flow as the base operations, plus sequence definitions and calls.
Small graph-changes and computations can be executed with sequence computations without the need to enter and exit again the rule layer.
The most notable feature of the sequences are their \emph{transactional} and \emph{backtracking constructs}, which support search and state space enumeration processes with their ability to roll changes to the graph back to an initial states, saving you a considerable amount of change reversal and bookkeeping code you'd need for implementing this on your own.

Expressiveness is achieved by language elements that were designed to be \emph{general} and \emph{orthogonal}.
\GrG\ offers the more general iterated patterns subsuming the multinodes as they are common for graph rewriting tools.
\GrG\ offers the more general recursive patterns subsuming the regular path expressions which are the more common choice.
You get means to \emph{combine patterns}, instead of dedicated solutions for single nodes or edges.
\GrG\ offers generic container types, and not only storages, as is more common (non-container-variables are even more common), and it offers them as graph element attributes, too and not only for processing variables.
\GrG\ offers not only node and edge types, but also a graph type allowing to store subgraphs; and especially it allows graph element attributes to be typed with it.
You can work with a type system as you know it from object-oriented-languages (where types can be used freely in defining other types).

Expressiveness is not for free, it's price is \emph{increased learning effort}.
\GrG\ offers the biggest toolbox for graph rewriting that you can find, with many tools and high-powered tools.
For nearly any graph-representation-based task do you find a collection of tools that allow you to handle it in an adequate way, leading to a \emph{concise solution} and \emph{low development} and \emph{maintenance times} and thus costs.
But you have to learn those tools and how to use them beforehand.
For small tasks that effort is likely higher than the net gains in the end.

For subtasks where the declarative pattern-based rules do not work well (which happens occasionally), are you free to revert to \emph{imperative} and \emph{object-oriented programming} as known from the C++, Java, C\# language family, utilizing the imperative parts of the \GrG-languages.
You loose a considerable amount of the benefits of employing the tool then (we consider model transformation tools offering own -- but only imperative -- languages pointless (and OCL-expressions are only a small improvement in this regard)), but neither are you stuck, nor are you forced to convoluted workarounds, as it may happen with tools that offer only pattern-based rules, e.g. Henshin\cite{Henshin} or AGG\cite{agg}.
The \emph{multi-paradigm languages} of \GrG\ offer you all the constructs you need for transforming graph-representations.

\subsection*{General-Purpose Graph Rewriting}
in contrast to \emph{special-purpose} graph rewriting requires high \emph{programmability}, \emph{genericity} and \emph{customizability}.

Some graph based tools are geared towards special application domains, e.g. biology (XL \cite{xl} or verification (GROOVE \cite{Groove}).
This means that design decisions were taken that ease uses in these application areas at the cost of rendering uses in other domains more difficult.
And that features were added in a way which just satisfy the needs of the domain at hand and its tasks instead of striving for a more general solution (which would have caused higher costs at designing and implementing this features).

This can be seen well in the approach towards state space enumeration. 
Instead of offering a built-in fixed-function state space enumerator like GROOVE and Henshin do, are you given the language devices needed to program one with a few lines of code, with the backtracking abilities of the sequences, and subgraph extraction, comparison, and storing in variables/graph element attributes.
This highlights the costs of being general-purpose: you \emph{must program} what you need.
But it also highlights the benefits: you \emph{can program exactly} what is needed, being very \emph{flexible} in how state-space enumeration is carried out.
The former tools expect a rule set which they apply exhaustively to create a state space, one state per rule application. 
But state space enumeration requires a lot of memory due to the state explosion phenomenon -- for practical applications you need control over which changes are to be seen as states and which not, and single rules often are too low-level regarding this classification, materializing many superfluous states.
\GrG\ gives you that kind of control (backtracking rule application and state materialization are separate things), allowing you to unfold deeper given the same amount of memory. 
When it comes to non-toy-examples, \emph{programmability wins} over coarse-grain \emph{fixed-function} units.
But of course only as long as you don't need to reinvent the wheel, when you can program based on a \emph{suitable level of abstraction}.

The backtracking constructs are useful in crawling a search space, too, where the reached states don't need to get materialized into the graph, and doing so would be inadequate.
When it comes to usability in many contexts, for many tasks, again programmability wins over coarse-grain fixed-function units.
This should not hinder you to choose the latter when your task does not require more, they are easier to use, but beware of the day when the requirements change and you hit a wall.
The approach of \GrG\ is to offer you \emph{languages} to achieve your goals, not \emph{pre-coded solutions} that may or may not fit to your task.
This holds not only for the languages used to describe the data or specify the computations, but also for the visualization and the debugger. They can be customized easily to the characteristics of your graph representation. 

Programmability always must come with means to abstract and encapsulate the programs into own units and parametrize them, so you are not exposed to details where you don't need them (but can access them when you need to do so, in contrast to opaque fixed-function units).
They are available with the rules and tests, the subpatterns, the procedures and functions (plus method procedures and method functions), the filter functions, and sequence definitions; and the input and output parameters available for them.

While the old \textsc{GrGen} started as a special-purpose compiler construction tool for internal use with fixed-shape patterns 
(optimizations on the graph based compiler intermediate representation FIRM -- see \url{www.libfirm.org}),
was the new \GrG\ built from the beginning as a general-purpose graph transformation tool for external use
-- to be used in areas as diverse as computer linguistics, engineering, computational biology or software engineering --
for reasoning with semantic nets, transformation of natural language to UML models,
model transformation, processing of program graphs, genome simulation, or pattern matching in social nets or RDF graphs.
Have a look at \cite{usecomputerlinguistics} or \cite{usemodeltransformation} or \cite{usegeneexpression} for some of the results.
Or at the results of \GrG\ for the diverse tasks posed in the transformation tool contests. 
It was always amongst the best rated tools, a record that is only achievable for a general-purpose tool with expressive and universally usable languages.
%which offers the highest combined speed of development and execution for graph based algorithms through its declarative languages with automatic optimization.

\subsection*{Performance}
i.e. high speed at modest memory consumption, is needed to tackle real world problems.
It is achieved by an \emph{optimized model implementation} that is tailored for efficient and scalable pattern matching (and rewriting),
while not growing memory needs out of proportion (as it happens for incremental match engines storing and maintaining the matches of all rules at any time).
The nodes and edges are organized in a system of ringlists, giving immediate access to the elements of a type, and to the incident elements of an element.
When needed, attribute indices or incidence count indices can be applied, giving fast access to graph elements based on attribute values, or incidence counts.

The generative approach and its \emph{compilation} of the rules into executable code is helping tremendously regarding performance, employing nested loops for matching the pattern elements (and a recursive descent based pushdown machine for combining the patterns).
But it comes at a cost: the test-debug-cycle is slowed down compared to an interpreter, and you are less flexible regarding runtime changes.
A further help regarding performance are the \emph{types}, which are speeding up the pattern matcher
-- besides being a help in modeling the domain and besides easening your life by eliminating large classes of errors at compile time.
Performance is further fostered by \emph{explicit control} with sequences, and rule applications from \emph{preset parameters} on, defining where to match the rules (giving rooted pattern matching), and which rule to match when (getting faster to a matching rule based on domain knowledge), in contrast to approaches based on implicit control, most notably the graph-grammar approach with its parameterless rules (only controlled by an optional layering). 

Performance is gained by the \emph{host graph sensitive search plans}.
In order to accelerate the matching step, we internally introduce \newterm{search plan}s
to represent different \newterm{matching strategies} and equip these search plans with a cost model, taking the present host graph into account.
The task of selecting a good search plan is then considered as an optimization problem~\cite{BKG:07,Bat:06}.
In contrast to systems like Fujaba\cite{fujaba,fuj}, our strongest competitor regarding performance,
is our pattern matching algorithm fully automatic and neither needs to be tuned nor partly be implemented by hand.

Furthermore, there are several strength reduction and inlining \emph{optimizations} employed by the compiler to eliminate the overhead of the higher level of abstraction where it is not needed;
in addition to the \emph{search state space stepping} that typically yields matches faster,
boosting rule applications executed from inside a loop,
which plays an important role in being at least one order of magnitude faster than any other tool known to us according to \indexed{Varr\'o's benchmark}\cite{varro_bench}.
Finally, search intensive tasks can be \emph{parallelized}, reaping benefits from our-days multicore machines; have a look at our solution for the Movie Database case of the TTC14\cite{MovieDatabase} to see what can be achieved this way.

\subsection*{Understandability and Learnability}
was taken care by evaluating for each language construct several options,
preferring constructs already known from imperative and object-oriented programming languages (as well as parser generators) ---
the ones which seemed most clean and intuitive while satisfying the other constraints were chosen.
This can be noted in comparison with the languages of the GReTL \cite{GReTL} tool, esp. with its powerful graph query language GReQL,
which may be pleasing to someone from the realm of formal specification, but which are not to the \emph{mind of a programmer}.
%A programmer who just wants to process his graphs on a higher level of abstraction, with declarative pattern matching and rewriting on a visualization of the network of objects -- instead of low level pointer structure fiddling, chasing objects by following references in the debugger.
You may have a look at the GrGen.NET solution of the Hello World! case \cite{HelloWorld} to judge on your own.
As we know that even the best designed language is not self explaining we put an emphasize on the \emph{user manual} currently read by you.
Especially since the consequences of the development goals expressiveness and programmability are inevitably increasing the learning effort.

\subsection*{Development Convenience}
is gained especially by the offered \emph{interactive} and \emph{graphical debugging} of rule applications.
The debugger visualizes the matched pattern and the changes carried out by rewriting in the graph where they apply,
for the currently active rule in the sequence (which is highlighted).

A further point easening development is the \emph{application programming interface} of the generated code,
which offers access to named, statically typed entities, catching most errors before runtime and allowing the code completion mechanisms of modern IDEs to excel.
In addition a generic interface operating on name strings and .NET objects is available for applications where the rules may change at runtime (as e.g. the \GrShell).

The API allows you to lookup graph elements by type, and to fetch all incident elements of an element, even in reverse direction, laying the foundation for graph-oriented programming.
In contrast to traditional programming that operates in passes over graph-representations from some root objects on, without direct access to elements inside the data structure. 
And with only the ability to follow an outgoing edge from its source node on, which means to dereference a pointer that points to the target.
Every node supports an unbounded number of incident edges, increasing flexibility und regularity compared to some statically fixed pointer fields as utilized normally.

There's one convenience not offered you might expect: a visual rule language and an editor.
This brings a clear benefit -- graph transformation specifications to be processed by \GrG\ can be easily \emph{generated} and \emph{textually diff'ed} --
but also is a good deal cheaper to implement.
Given the limited resources of an university or open-source project this is an important point,
as can be seen with the AGG\cite{agg} tool, offering a graphical editor but delivering performance only up to simple toy examples
--- unfortunately causing the wrong impression that graph rewriting is notoriously inefficient.

\subsection*{Well Founded Semantics}
to ease formal, but especially human reasoning.
The semantics of \GrG\ are specified in \cite{DissRuby}, based upon graph homomorphisms, denotational evaluation functions and category theory.
The \GrG-rewrite step is based by default on the \newterm{single-pushout approach} (SPO, for explanation see~\cite{spoapproach}),
with the \newterm{double-pushout approach} (DPO, for explanation see~\cite{dpoapproach}) available on request, too.
The semantics of the recursive rules introduced in version 2.0 are given in \cite{Jak:08},
utilizing pair star graph grammars on the meta level to assemble the rules of the object level.
The formal semantics are not as complete as for the graph programming language GP\cite{gp} though, mainly due to the large amount of features
--- the convenience at using the language had priority over the convenience at reasoning formally about the language.

\subsection*{Platform Independence}
is achieved by using languages compiled to byte code for virtual machines backed by large, standardized libraries, specifically: Java and C\#.
This should prevent the fate of the grandfather of all graph rewrite systems, PROGRES\cite{schuerr99progres},
which achieved a high level of sophistication, but is difficult to run by now, or simply: outdated.

There's another trait to platform independence: \GrG\ is built on its own model layer, in contrast to the JAVA model transformation world, where EMF has emerged as a quasi-standard.
Don't expect this to change, the performance of declarative pattern matching and rewriting is our top concern, and that can be best achieved with a model optimized for this task (the current type and neighbourhood ringlist implementation is efficient and scalable, and supports the search space stepping optimization; directed edges are always navigable in both directions, giving the planner more room for choosing a good search plan; and some fields are included that render isomorphy checking cheap and supply some cost-free visited flags).

EMF is not that important for us for another reason: we consider the MDA vision an illusion.
The costs of developing a domain-specific language, a model as a high-level representation of a domain, and a code generator emitting executable code from it (maybe with an intermediate platform dependent model in the chain) are considerable and can only be amortized over many use cases or for large program families, with a high degree of commonalities (otherwise there's nothing domain-specific to exploit) and a high degree of variation (otherwise there would be no real benefit compared to direct coding).
Model-based development of single applications is economical nonsense, no matter what the claimed benefits are, the costs will never pay off.
So only large program families are left, which are few, or languages for certain infrastructure tasks that are common among multiple applications, which requires a general-purpose language for that specific domain so it can be adapted to the exact context, leading to further increased costs (as can be seen in the feature development of the GrGen language, a domain-specific language for graph-representation processing...).
We assume there are a good deal more use cases existing for graph representation processing than for model-based development.

\subsection*{General-Purpose Graph Transformation}
in contrast to \emph{model} transformation.
Several model transformation tools offer graph pattern matching as a means of transformation and graph-based tools have been successfully used to model many domains; a clear separation is not possible then \cite{Jakumeit2013}.
But pattern matching is typically limited in dedicated model transformation tools, as is their performance regarding it, esp. their ability for achieving automatically high-performance solutions --- the EMF poses an obstacle to efficient automatic-declarative pattern matching.
Often, model transformation tools offer just \emph{mapping single elements} of a source model (inspecting some local context via OCL-expressions) to some target elements, which is only sufficient for simple task.
Those tools -- e.g. ATL\cite{atl}, to a lesser degree Epsilon\cite{epsilon} -- typically offer only \emph{batch-wise offline} mapping of one graph-like representation to another one.
\GrG\ works well for model transformation task, esp. due to its compiler construction roots; so feel free to use it if you are one of the few that indeed benefit directly from the model driven architecture approach or are interested in supplying corresponding models and languages to the benefit of others. 

Furthermore, graph \emph{transformation} in contrast to graph \emph{databases}.
The focus is on efficient \emph{pattern-based} matching, and rewritings that are strongly coupled to the query results, capable of executing fine-grain spot-wise changes (with transaction support for crawling search spaces). 
Executed on an \emph{in-memory} graph-structure, by a \emph{single user} at a time.
In contrast to e.g. Neo4J\cite{neo}, that offers non-pattern queries that are feeding loosely coupled simple updates via a query-result container in between, which is accumulating the matches but renders fine-grain changes based on the exact matches/spots difficult.
Executed on a \emph{backing-store}, by \emph{multiple users} that are working concurrently at the same time, isolated by transactions.

So \GrG\ is not suited for scenarios where many users need to access data at the same time. 
It is very fast, so you can handle multiple-user scenarios simply by serializing the accesses --- this could be well sufficient for your tasks, but won't scale towards large user numbers.
You may use it as an application-embedded database, as
\GrG\ \emph{does} offer \emph{online} modification of graph-structures,
but persistence of online modifications is only offered via recording to a change log that needs to be replayed at the next start
(besides writing a full dump from memory at session end, as it is common to batch-wise offline processing).

\GrG\ is not suited for scenarios where the limits of main memory are exceeded.
A well equipped single sever with 128GB of main memory is capable of storing somewhat under about 500 Million nodes plus 1 Billion edges (naked, without attributes and without names), see Chapter \ref{sec:performance} for the maths.
This should be sufficient for many tasks, but still there are task that grow beyond it.
And unfortunately other things come into play at that sizes -- importing such a graph will take several hours (the GRS importer is munching at some ten thousand elements per second), databases that work on a backing store and only shuffle the data visited in are at an advantage here (amortizing import cost over the course of processing).
\GrG\ scales very well and can easily handle millions of elements, matching a pattern in such a graph within the blink of an eye, but it does not contain dedicated optimizations for very huge datasets.

\GrG\ is a tool that was built for achieving goals in practice in mind.
Despite a shared pair graph grammer inspiration is it unrelated to triple graph grammars and their implicit assumption that typically one graph representation is to be transformed to another basically equivalent, just somehow a bit differently formulated graph representation, without information loss or enrichement in either direction (bidirectional transformation tasks are very seldom).
\GrG\ can be used for meta-programming (esp. due ot its compiler construction roots), but otherwise do you work with it at the object and class level, not at the meta, meta-meta, or meta-meta-meta tralala-ding-ding-dong level (the abstraction levels of the real model-based software engineers).
When you don't care about expressiveness and performance for real world taks, but are just interested in pimping your thesis, pick AGG and do some critical pair analysis.
\GrG\ was not created to prove that algebraic graph transformation can be casted into program form, neither was it created to prove a visual programming point.
\GrG\ was created because we know from our own experience that a great graph processing language and a supporting environment with visual debugging are of real help for graph-representation-based tasks.
